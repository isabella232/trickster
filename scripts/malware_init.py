#!/usr/bin/env python3

import sys
sys.path.append('..')

# Ignore warnings.
import warnings
warnings.filterwarnings('ignore')

# Handle library imports.
import logging
import numpy as np
import pandas as pd
from os import listdir
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegressionCV
from scipy.sparse import csr_matrix, save_npz, load_npz

from trickster.search import a_star_search, ida_star_search

###########################################
###########################################
###########################################

# Handle global variables.

MANIFEST_FEATURES = [    # corresponds to...
    'provider',          # "Hardware Components" 
    'permission',        # "Permissions"
    'activity',          # "Components" (185,729 / 218,951)
    'service_receiver',  # "Components" (33,222  / 218,951)
    'intent'             # "Intents"
]                        # ... in the Grosse et al. paper

MANIFEST_FEATURES_IDXS = None

SEED = 2018

###########################################
###########################################
###########################################

# Define useful helper functions.

def load_data(data_folder, hashes_csv, subset=None):
    
    # Record hashes corresponding to malware applications.
    df = pd.read_csv(hashes_csv)
    malware_hashes = set(df['sha256'])
    
    # Load the data and record the feature set.
    data, labels, features = [], [], set()
    if subset is None:
        file_paths = listdir(data_folder)
    else:
        file_paths = listdir(data_folder)[:subset]
        
    for file_path in file_paths:
        with open(data_folder + file_path) as f:
            lines = [x.strip() for x in f]
            if lines == '':
                continue
            data.append(lines)
            features |= set(lines)
            label = 1 if file_path in malware_hashes else 0
            labels.append(label)
            
    return data, labels, features

def fit_transform(data, features):
    
    # Fit a label encoder and transform the input data.
    label_encoder = LabelEncoder()
    label_encoder.fit(list(features))
    encoded = [label_encoder.transform(x) for x in data]
    return encoded, label_encoder

def process_data(encoded, labels, features):
    
    # Create a sparse binary matrix from encoded data.
    indptr = np.cumsum([0] + [len(x) for x in encoded])
    indices = np.concatenate(encoded)
    ones = np.ones(indices.size)

    N, K = len(data), len(features)
    X = csr_matrix((ones, indices, indptr), shape=(N, K))
    y = np.array(labels)   
    return X, y

def fit_validate(X_train, y_train):
    
    # Fit logistic regression by performing a Grid Search with Cross Validation.
    Cs = np.arange(0.5, 1.5, 0.025)
    class_weight = 'balanced' # balanced or None
    scoring = 'accuracy' # accuracy or roc_auc

    clf = LogisticRegressionCV(
        Cs=Cs, 
        cv=5, 
        n_jobs=-1, 
        penalty='l2',
        scoring=scoring,
        class_weight=class_weight,
        random_state=SEED
    )
    
    clf.fit(X_train, y_train)
    return clf

def get_manifest_map(label_encoder):    
    manifest_map = {}
    for i, c in enumerate(label_encoder.classes_):
        feature_class = c.split('::')[0]
        manifest_map[i] = feature_class in MANIFEST_FEATURES
    return manifest_map

###########################################
###########################################
###########################################

# Define useful helper classes.

class LogisticRegressionScikitSaliencyOracle:
    
    def __init__(self, model):
        self.model = model

    def eval(self, _):
        return self.model.coef_[0]
    
    
class Node:
    
    def __init__(self, x):
        self.root = x

    def expand(self):
        """Generate all children of the current node."""

        children = []
                
        for feat_idx in MANIFEST_FEATURES_IDXS:
            
            # Skip if the feature is already set.
            if self.root[feat_idx] == 1:
                continue
                
            child = np.array(self.root)
            child[feat_idx] = 1
            children.append(child)
            
        return children
    
    def __repr__(self):
        return 'Node({})'.format(self.root)

###########################################
###########################################
###########################################

# Provide implemention of Algorithm 1 from Grosse et al. paper.

def find_adversarial_grosse(x, clf, oracle, manifest_map, k=20, return_path=False):
    
    if clf.predict([x]) == 0:
        raise Exception('Initial example is already classified as bening.')
        
    if return_path:
        path = [x]
        
    x_star = np.array(x, dtype='intc')
    distortions = 0
    
    while clf.predict([x_star]) != 0 and distortions < k:
        derivative = oracle.eval(x_star)
        idxs = np.argsort(derivative)
        
        for i, idx in enumerate(idxs):
            
            # Check if changing the feature is permitted.
            if x_star[idx] == 0 and manifest_map[idx]:
                x_star[idx] = 1
                if return_path:
                    path.append(np.array(x_star))
                break
                
            if i == len(idxs) - 1:
                raise Exception('Adversarial example is impossible to create.')
                
        distortions += 1
        
    if distortions == k:
        raise Exception('Distortion bound reached.')
        
    if return_path:
        return x_star, distortions, path
    else:
        return x_star, distortions
    
    
# Provide implemention of our algorithm using heuristic and A* search.

def find_adversarial(x, clf, search_fn, p_norm=1, q_norm=np.inf,
                     target_confidence=0.5, return_path=False, **kwargs):
    """Transform an example until it is classified with target confidence.""" 
    
    def expand_fn(x, p_norm=1, **kwargs):
        """Wrap the example in `Node`, expand the node, and compute the costs.

        Returns a list of tuples (child, cost)
        """
        node = Node(x, **kwargs)
        children = node.expand()
        costs = [np.linalg.norm(x - c, ord=p_norm) for c in children]         
        return list(zip(children, costs))

    def goal_fn(x, clf, target_confidence=0.5):
        """Tell whether the example has reached the goal."""
        return clf.predict_proba([x])[0, 1] <= target_confidence

    def heuristic_fn(x, clf, q_norm=np.inf):
        """Distance to the decision boundary of a logistic regression classifier.

        By default the distance is w.r.t. L1 norm. This means that the denominator
        has to be in terms of the Holder dual norm (`q_norm`), so L-inf. I know,
        this interface is horrible.

        NOTE: The value has to be zero if the example is already on the target side
        of the boundary.
        """
        score = clf.decision_function([x])[0]
        if score <= 0:
            return 0.0
        h = np.abs(score) / np.linalg.norm(clf.coef_[0, MANIFEST_FEATURES_IDXS], ord=q_norm)    
        return h

    def hash_fn(x):
        """Hash function for examples."""
        return hash(x.tostring())

    if clf.predict_proba([x])[0, 1] <= target_confidence:
        raise Exception('Initial example is already classified as bening.')        
    return search_fn(
        start_node=x, 
        expand_fn=lambda x: expand_fn(x, p_norm=p_norm, **kwargs), 
        goal_fn=lambda x: goal_fn(x, clf, target_confidence), 
        heuristic_fn=lambda x: heuristic_fn(x, clf, q_norm=q_norm), 
        hash_fn=hash_fn,
        return_path=return_path
    )

###########################################
###########################################
###########################################

# Main function
if __name__ == "__main__":
    
    # Set up a logger object to print info to stdout.
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    
    # Load the data and record the feature set.
    logger.info(">> Loading data from DREBIN dataset...")
    data_folder = '../data/drebin/'
    hashes_csv = '../data/drebin_malware_sha256.csv'
    data, labels, features = load_data(data_folder, hashes_csv, subset=1000)
    
    # Fit a label encoder and transform the input data.
    logger.info(">> Label encoding input data...")
    encoded, label_encoder = fit_transform(data, features)
    
    # Prepare input data for learning.
    logger.info(">> Preparing data for learning...")
    X, y = process_data(encoded, labels, features)
    
    logger.info(">> Bening samples: {}. Malware samples: {}. Total: {}.".format(y.size - sum(y), sum(y), y.size))
    
    # Split into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
    # Fit logistic regression by performing a Grid Search with Cross Validation.
    logger.info(">> Fitting logistic regression...")
    clf = fit_validate(X_train, y_train)
    
    logger.info(">> Resulting training accuracy is: {:.2f}%. Test accuracy is: {:.2f}%."
                .format(clf.score(X_train, y_train)*100, clf.score(X_test, y_test)*100))
    
    # Set indexes for the features found in the Android manifest.
    manifest_map = get_manifest_map(label_encoder) 
    MANIFEST_FEATURES_IDXS = [f for (f,b) in manifest_map.items() if b]
    
    # Run experiments to compare Grosse et al. with JSMA against our heuristic.
    logger.info(">> Comparing JSMA against our heuristic...")
    oracle = LogisticRegressionScikitSaliencyOracle(clf)

    for i, x in enumerate(X):

        # Transform the example from compressed matrix format to numpy array.
        x = x.toarray()[0] 

        if clf.predict([x]) == 1:
            logger.info('>> Crafting adversarial example for example: {}...'.format(i))

            # Try finding adversarial example using JSMA with distortion bound k = 100.
            try:
                x_adv_grosse, cost_grosse = find_adversarial_grosse(x, clf, oracle, manifest_map, k=100)
            except Exception as e:
                logger.info('>> JSMA failed! {}'.format(e))
                continue

            # Try finding adversarial example using our heuristic with A* search.
            x_adv, cost = find_adversarial(x, clf, a_star_search, p_norm=1, q_norm=np.inf)
            
            assert clf.predict([x_adv_grosse]) == 0 and clf.predict([x_adv]) == 0

            logger.info('>> Cost using JSMA: {}. Using our heuristic: {}.'.format(cost_grosse, int(cost)))
    
    
    
    
